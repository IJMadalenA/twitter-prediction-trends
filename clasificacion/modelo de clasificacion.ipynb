{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "#y_pred = cross_val_predict(clf, x, y, cv=10)\n",
    "#conf_mat = confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(string):\n",
    "    if string != \"[]\":\n",
    "        string = json.loads(string.replace(\"'\", \"\\\"\"))\n",
    "        return \",\".join([s[\"screen_name\"] for s in string])\n",
    "    return \"\"\n",
    "\n",
    "def to_list(list_):\n",
    "    if list_ != \"[]\":\n",
    "        list_ = list_[1:-1]\n",
    "        list_ = list_.split(\",\")\n",
    "        return \",\".join([s.strip().strip(\"'\") for s in list_])\n",
    "    return \"\"\n",
    "\n",
    "def normalize(s):\n",
    "    replacements = ((\"á\", \"a\"), (\"é\", \"e\"), (\"í\", \"i\"), (\"ó\", \"o\"), (\"ú\", \"u\"))\n",
    "    for a, b in replacements:\n",
    "        s = s.lower()\n",
    "        s = s.replace(a, b)\n",
    "    return s\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U0000270D\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r\"\", text)\n",
    "\n",
    "def cleanTxt(text):\n",
    "    text = re.sub(r\"@[a-zA-Z0-9]+\", \"\", text) #Removes @mentions\n",
    "    text = re.sub(r\"#\", \"\", text) #Removing the \"#\" symbol\n",
    "    text = re.sub(r\"RT[\\s]+\", \"\", text) #Removing RT\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\", \"\", text) #Remove the hyperlink\n",
    "    return text\n",
    "\n",
    "def replace_punct(s):\n",
    "    for i in string.punctuation:\n",
    "        if i in s:\n",
    "            s = s.replace(i, \"\").strip()\n",
    "    return s\n",
    "\n",
    "def replace_num(s):\n",
    "    for i in [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]:\n",
    "        s = s.replace(i, \"\")\n",
    "    return s\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub(r\"[\\W]+\", \"\", text.lower()) \n",
    "    return text\n",
    "\n",
    "def tokenizador(text):\n",
    "    important_words = []\n",
    "    for word in text.split(\" \"):\n",
    "        if word not in stopwords.words(\"spanish\"):\n",
    "            if word != \"\":\n",
    "                important_words.append(word)\n",
    "    return \" \".join(important_words).strip()\n",
    "\n",
    "def foo(text):\n",
    "    forbidden = (\"?\", \"¿\", \"¡\", \"!\", \",\", \".\", \";\", \":\", \"-\", \"'\", \"+\", \"$\", \"/\", \"*\",'«','»', \"~\", \"(\", \")\")\n",
    "    aux = \"\"\n",
    "    for v in text:\n",
    "        if not v in forbidden:\n",
    "            aux += v\n",
    "    return aux\n",
    "\n",
    "def quitar_abreviaciones(text):\n",
    "    abreviaciones = {\"ue\" : \"union europea\", \n",
    "                     \"pp\" : \"partido popular\", \n",
    "                     \"tc\" : \"tribunal constitucional\", \n",
    "                     \"no\" : \"no\",\n",
    "                     \"si\" : \"si\", \n",
    "                     \"iu\" : \"izquierda unida\", \n",
    "                     \"cs\" : \"ciudadanos\"}\n",
    "    aux = \"\"\n",
    "    for word in text.split(\" \"):\n",
    "        if word in abreviaciones.keys():\n",
    "            aux += abreviaciones[word] + \" \"\n",
    "        else: aux += word + \" \"\n",
    "    return aux.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moda = pd.read_csv(\"C:/Users/Daniel/Documents/GitHub/SuperProyecto/clasificacion/clasificador_moda.csv\")\n",
    "df_moda.drop(df_moda.columns[0], axis = 1, inplace = True)\n",
    "#df_moda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Daniel/Documents/GitHub/SuperProyecto/clasificacion/elmundo2.csv\")\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"conversation_id\", \"cashtags\", \"timezone\", \"user_id\", \"name\", \"near\", \"geo\", \"source\",\n",
    "                   \"user_rt_id\", \"user_rt\", \"retweet_id\", \"retweet_date\", \"translate\", \"trans_src\",\n",
    "                   \"trans_dest\", \"place\", \"quote_url\", \"thumbnail\", \"created_at\", \"id\", \"link\"]\n",
    "\n",
    "df.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "df = df[df.language == \"es\"]\n",
    "\n",
    "df.drop(\"language\", axis = 1, inplace = True)\n",
    "\n",
    "df = df.reset_index().drop(\"index\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.photos = df.photos.apply(lambda x : 1 if x != \"[]\" else 0)\n",
    "df.retweet = df.retweet.apply(lambda x : 1 if x == \"True\" else 0)\n",
    "df.urls = df.urls.apply(lambda x : 1 if x != \"[]\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df_moda.iloc[:, -1]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.tweet = df.tweet.apply(normalize)\n",
    "df.tweet = df.tweet.apply(deEmojify)\n",
    "df.tweet = df.tweet.apply(cleanTxt)\n",
    "df.tweet = df.tweet.apply(replace_punct)\n",
    "df.tweet = df.tweet.apply(replace_num)\n",
    "\n",
    "df.tweet = df.tweet.apply(tokenizador)\n",
    "df.tweet = df.tweet.apply(foo)\n",
    "df.tweet = df.tweet.apply(quitar_abreviaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_stopwords = []\n",
    "for word in set(\" \".join([tweet for tweet in df.tweet]).split(\" \")):\n",
    "    if len(word) <= 2 and word not in [\"si\", \"no\"]:\n",
    "        mas_stopwords.append(word)\n",
    "        \n",
    "def quitar_mas_stopwords(text):\n",
    "    aux = \"\"\n",
    "    for word in text.split(\" \"):\n",
    "        if word not in mas_stopwords:\n",
    "            aux += word + \" \"\n",
    "    return aux.strip()\n",
    "\n",
    "df.tweet = df.tweet.apply(quitar_mas_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_unicas = []\n",
    "for tweet in df.tweet:\n",
    "    palabras_unicas.extend(tweet.split(\" \"))\n",
    "palabras_unicas = set(palabras_unicas)\n",
    "palabras_unicas = {palabras : 0 for palabras in palabras_unicas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "todos = []\n",
    "for tweet in df.tweet:\n",
    "    tweet_dic = palabras_unicas.copy()\n",
    "    for word, count in Counter(tweet.split(\" \")).items():\n",
    "        tweet_dic[word] = count\n",
    "    todos.append(tweet_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "todos_df = pd.DataFrame(todos, index = range(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "todos_df = tfidf.fit_transform(todos_df).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.DataFrame(todos_df, index = range(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop([\"date\", \"time\", \"username\", \"tweet\", \"mentions\", \"hashtags\", \"reply_to\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([matrix, df2], axis = 1)\n",
    "#df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop(\"moda\", axis = 1)\n",
    "y = df2.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 6618), (2000,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_model         = LogisticRegression(solver = \"liblinear\")\n",
    "#nearest_centroid  = NearestCentroid()\n",
    "#knn_classifier    = KNeighborsClassifier(11)\n",
    "#radius_classifier = RadiusNeighborsClassifier(1)\n",
    "#tree_classifier   = DecisionTreeClassifier() \n",
    "#forest_classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#log     = cross_validate(log_model,         X, y, cv = folds, scoring = make_scorer(accuracy_score))\n",
    "#nearest = cross_validate(nearest_centroid,  X, y, cv = folds, scoring = make_scorer(accuracy_score)) \n",
    "#knn     = cross_validate(knn_classifier,    X, y, cv = folds, scoring = make_scorer(accuracy_score))\n",
    "#radius  = cross_validate(radius_classifier, X, y, cv = folds, scoring = make_scorer(accuracy_score))\n",
    "#tree    = cross_validate(tree_classifier,   X, y, cv = folds, scoring = make_scorer(accuracy_score))\n",
    "#forest  = cross_validate(forest_classifier, X, y, cv = folds, scoring = make_scorer(accuracy_score))\n",
    "\n",
    "#models_scores_table = pd.DataFrame({\"Logistic Regression\" : [log[\"test_accuracy\"].mean()],\n",
    "#                                    \"Nearest centroid\"    : [nearest[\"test_accuracy\"].mean()],\n",
    "#                                    \"KNN Classifier\"      : [knn[\"test_accuracy\"].mean()],\n",
    "#                                    \"Radius Classifier\"   : [radius[\"test_accuracy\"].mean()],\n",
    "#                                    \"Decision Tree\"       : [tree[\"test_accuracy\"].mean()],\n",
    "#                                    \"Random Forest\"       : [forest[\"test_accuracy\"].mean()]},\n",
    "#                                    index = [\"Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models_scores_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    47.1\n",
       "1.0    35.6\n",
       "2.0    10.0\n",
       "3.0     7.3\n",
       "Name: moda, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()/2000*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_classifier.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
