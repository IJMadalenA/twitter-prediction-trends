{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    replacements = ((\"á\", \"a\"), (\"é\", \"e\"), (\"í\", \"i\"), (\"ó\", \"o\"), (\"ú\", \"u\"))\n",
    "    for a, b in replacements:\n",
    "        s = s.lower()\n",
    "        s = s.replace(a, b)\n",
    "    return s\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U0000270D\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r\"\", text)\n",
    "\n",
    "def cleanTxt(text):\n",
    "    text = re.sub(r\"@[a-zA-Z0-9]+\", \"\", text) #Removes @mentions\n",
    "    text = re.sub(r\"#\", \"\", text) #Removing the \"#\" symbol\n",
    "    text = re.sub(r\"RT[\\s]+\", \"\", text) #Removing RT\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\", \"\", text) #Remove the hyperlink\n",
    "    return text\n",
    "\n",
    "def replace_punct(s):\n",
    "    for i in string.punctuation:\n",
    "        if i in s:\n",
    "            s = s.replace(i, \"\").strip()\n",
    "    return s\n",
    "\n",
    "def replace_num(s):\n",
    "    for i in [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]:\n",
    "        s = s.replace(i, \"\")\n",
    "    return s\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub(r\"[\\W]+\", \"\", text.lower()) \n",
    "    return text\n",
    "\n",
    "def tokenizador(text):\n",
    "    important_words = []\n",
    "    for word in text.split(\" \"):\n",
    "        if word not in stopwords.words(\"spanish\"):\n",
    "            if word != \"\":\n",
    "                important_words.append(word)\n",
    "    return \" \".join(important_words).strip()\n",
    "\n",
    "def foo(text):\n",
    "    forbidden = (\"?\", \"¿\", \"¡\", \"!\", \",\", \".\", \";\", \":\", \"-\", \"'\", \"+\", \"$\", \"/\", \"*\",'«','»', \"~\", \"(\", \")\")\n",
    "    aux = \"\"\n",
    "    for v in text:\n",
    "        if not v in forbidden:\n",
    "            aux += v\n",
    "    return aux\n",
    "\n",
    "def quita_palabras_pequeñas(text):\n",
    "    return \" \".join([word for word in text.split(\" \") if len(word) > 2])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"C:/Users/Daniel/Desktop/csv/no_trending.csv\")\n",
    "df.drop(df.columns[0], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17332, 17)\n"
     ]
    }
   ],
   "source": [
    "# Filtro por el dia 24 o 25\n",
    "\n",
    "FECHA = 24\n",
    "\n",
    "df = df[df.day == FECHA]\n",
    "df = df.reset_index()\n",
    "df.drop(df.columns[0], axis = 1, inplace = True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierto la columna hashtags a str\n",
    "df.hashtags = df.hashtags.apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 107 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Eliminio las filas que no tengan texto en el tweet\n",
    "\n",
    "tweet_na = []\n",
    "\n",
    "for num, tweet in enumerate(df.tweet):\n",
    "    if type(tweet) != str:\n",
    "        tweet_na.append(num)\n",
    "\n",
    "\n",
    "df.drop(tweet_na, inplace = True)\n",
    "\n",
    "df = df.reset_index()\n",
    "df.drop(df.columns[0], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo las tendencias de ese dia\n",
    "\n",
    "tendencias = []\n",
    "with open(\"C:/Users/Daniel/Desktop/csv/dia 24/trends/dia 24 tendencias.txt\", \"r\") as f:\n",
    "    tendencias.extend(f.readlines())\n",
    "    \n",
    "tendencias = [t[:-1].strip(\"/t\") for num, t in enumerate(tendencias) if num != len(tendencias) - 1][-50:]\n",
    "\n",
    "df_tendencias = pd.DataFrame(tendencias, columns = [\"trends\"])\n",
    "df_tendencias = df_tendencias.trends.unique()\n",
    "df_tendencias = pd.DataFrame(df_tendencias, columns = [\"trends\"])\n",
    "solo_tendencias = list(df_tendencias.trends.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtags_tendencias: 26\n",
      "palabras_tendencias: 24\n",
      "hashtags_tendencias_sin_numeral: 26\n",
      "palabras_tendencias_lower: 24\n"
     ]
    }
   ],
   "source": [
    "# Lista de palabras tendencias y hashtags tendencias\n",
    "\n",
    "hashtags_tendencias = [t for t in solo_tendencias if t[0] == \"#\"]\n",
    "palabras_tendencias = [t.strip(\"\\t\") for t in solo_tendencias if t[0] != \"#\"]\n",
    "hashtags_tendencias_sin_numeral = [t.strip(\"#\").lower() for t in solo_tendencias if t[0] == \"#\"]\n",
    "palabras_tendencias_lower = [t.strip(\"\\t\").lower() for t in solo_tendencias if t[0] != \"#\"]\n",
    "\n",
    "print(\"hashtags_tendencias:\", len(hashtags_tendencias))\n",
    "print(\"palabras_tendencias:\", len(palabras_tendencias))\n",
    "print(\"hashtags_tendencias_sin_numeral:\", len(hashtags_tendencias_sin_numeral))\n",
    "print(\"palabras_tendencias_lower:\", len(palabras_tendencias_lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HASHTAGS NO TENDENCIA MAS REPETIDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de hashtasg no tendencia: 26\n",
      "Wall time: 85 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Cuanto cuantos hashtags hay en el df y me quedo con los mas repetidos\n",
    "\n",
    "hashtags_no_tendencias = list()\n",
    "for h in df.hashtags:\n",
    "    for hashtag in h.split(\",\"):\n",
    "        if hashtag not in hashtags_tendencias and hashtag != \"nan\":\n",
    "            hashtags_no_tendencias.append(hashtag)\n",
    "                \n",
    "hashtags_no_tendencias = Counter(hashtags_no_tendencias).most_common()[:len(hashtags_tendencias_sin_numeral)]\n",
    "hashtags_no_tendencias = {h[0] : h[1] for h in hashtags_no_tendencias}\n",
    "\n",
    "print(\"Numero de hashtasg no tendencia:\", len(hashtags_no_tendencias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tweets con hashtags tendencias: 40\n",
      "Wall time: 151 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Saco los indices de las filas que tengan hashtags tendencias\n",
    "\n",
    "hashtags_indices = []\n",
    "for num, h in enumerate(df.hashtags):\n",
    "    for hashtag in h.split(\",\"):\n",
    "        if hashtag.lower() in hashtags_tendencias_sin_numeral:\n",
    "            hashtags_indices.append(num)\n",
    "                \n",
    "print(\"Cantidad de tweets con hashtags tendencias:\", len(hashtags_indices))\n",
    "\n",
    "df.drop(hashtags_indices, inplace = True)\n",
    "\n",
    "df = df.reset_index()\n",
    "df.drop(df.columns[0], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PALABRAS CLAVES NO TENDENCIA MAS REPETIDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "Wall time: 1.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Voy a quitar los tweets que tengan palabras claves tendencias\n",
    "\n",
    "palabras_indices = []\n",
    "\n",
    "for num, tweet in enumerate(df.tweet):\n",
    "    for palabra in palabras_tendencias_lower:\n",
    "        if tweet.lower().find(palabra) != -1:\n",
    "            palabras_indices.append(num)\n",
    "\n",
    "print(len(palabras_indices))\n",
    "            \n",
    "df.drop(palabras_indices, inplace=True)\n",
    "\n",
    "df = df.reset_index().drop(df.columns[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Ahora voy a limpiar los tweets, para poder ver que palabras claves no tendencia se repiten mas\n",
    "\n",
    "df.tweet = df.tweet.apply(normalize)\n",
    "df.tweet = df.tweet.apply(deEmojify)\n",
    "df.tweet = df.tweet.apply(cleanTxt)\n",
    "df.tweet = df.tweet.apply(replace_punct)\n",
    "df.tweet = df.tweet.apply(replace_num)\n",
    "df.tweet = df.tweet.apply(quita_palabras_pequeñas)\n",
    "\n",
    "df.tweet = df.tweet.apply(tokenizador)\n",
    "df.tweet = df.tweet.apply(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965\n",
      "Wall time: 104 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Dropeo las filas de tweets que tengan texto \"\"\n",
    "\n",
    "tweet_vacios = []\n",
    "\n",
    "for num, tweet in enumerate(df.tweet):\n",
    "    if tweet == \"\":\n",
    "        tweet_vacios.append(num)\n",
    "\n",
    "print(len(tweet_vacios))        \n",
    "\n",
    "df.drop(tweet_vacios, inplace = True)\n",
    "\n",
    "df = df.reset_index()\n",
    "df.drop(df.columns[0], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "Wall time: 276 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Cuanto cuantos palabras hay en el df y me quedo con los mas repetidos\n",
    "\n",
    "palabras_no_tendencias = list()\n",
    "for p in df.tweet:\n",
    "    for palabra in p.split(\" \"):\n",
    "        palabras_no_tendencias.append(palabra)\n",
    "            \n",
    "palabras_no_tendencias = Counter(palabras_no_tendencias).most_common()[:len(palabras_tendencias)]\n",
    "palabras_no_tendencias = {h[0] : h[1] for h in palabras_no_tendencias}\n",
    "\n",
    "print(len(palabras_no_tendencias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHORA SEPARAMOS EL DF: LOS QUE TIENE HASHTAGS Y CON LOS QUE NO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF_H (df de los hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h = df[df.hashtags != \"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df_h.drop(df_h.columns[0], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h = df_h.reset_index()\n",
    "df_h.drop(df_h.columns[0], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h[\"trends\"] = [[h if h in hashtags_no_tendencias else 0 for h in hashtag.split(\",\")] for hashtag in df_h.hashtags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_drop = list()\n",
    "for num, t in enumerate(df_h.trends):\n",
    "    if 0 in t:\n",
    "        indices_drop.append(num)\n",
    "        \n",
    "df_h.drop(indices_drop, inplace = True)\n",
    "\n",
    "df_h = df_h.reset_index()\n",
    "df_h.drop(df_h.columns[0], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices_para_clonar = list()\n",
    "for num, t in enumerate(df_h.trends):\n",
    "    if len(t) > 1:\n",
    "        indices_para_clonar.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dic_indices = {indice : [len(trends), trends] for indice, trends in zip(indices_para_clonar, df_h.loc[indices_para_clonar].trends)}\n",
    "\n",
    "df_v = pd.DataFrame(columns = df_h.columns)\n",
    "\n",
    "for key in dic_indices.keys():\n",
    "    for time in range(dic_indices[key][0]):\n",
    "        df_d = pd.DataFrame(df_h.loc[key]).T\n",
    "        df_d.drop(df_d.columns[-1], axis = 1, inplace = True)\n",
    "        df_d[\"trends\"] = dic_indices[key][1][time]\n",
    "        df_v = pd.concat([df_v, df_d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h.drop(indices_para_clonar, inplace = True)\n",
    "\n",
    "df_h = df_h.reset_index()\n",
    "df_h.drop(df_h.columns[0], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h.trends = df_h.trends.apply(lambda x : x[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>mentions</th>\n",
       "      <th>urls</th>\n",
       "      <th>photos</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet</th>\n",
       "      <th>video</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>hours</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>trends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23:50:56</td>\n",
       "      <td>nanoflores75</td>\n",
       "      <td>yosoydelosmillonesdemisterchip datos puros dur...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yosoydelos3millonesdemisterchip</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>yosoydelos3millonesdemisterchip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23:50:48</td>\n",
       "      <td>miguelsvl</td>\n",
       "      <td>seguir viendo datos reirme contestaciones das ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yosoydelos3millonesdemisterchip</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>yosoydelos3millonesdemisterchip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23:50:02</td>\n",
       "      <td>justseero</td>\n",
       "      <td>datos yosoydelosmillonesdemisterchip</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yosoydelos3millonesdemisterchip</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010MisterChip</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>yosoydelos3millonesdemisterchip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23:48:01</td>\n",
       "      <td>pisanijavier</td>\n",
       "      <td>yosoydelosmillonesdemisterchip informacion pod...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yosoydelos3millonesdemisterchip</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>yosoydelos3millonesdemisterchip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23:46:38</td>\n",
       "      <td>fcolopezm_</td>\n",
       "      <td>coronavirus españa positivo canarias cuatro ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>coronavirus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>01:39:06</td>\n",
       "      <td>ostwaldguillen</td>\n",
       "      <td>maialen favorita maialen favorita maialen favo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>139</td>\n",
       "      <td>otgala6,otchat6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>otchat6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>01:34:27</td>\n",
       "      <td>saramatarile</td>\n",
       "      <td>parece frivolo dejeis darle abrazo intimidad p...</td>\n",
       "      <td>soyanneot2020,ot_oficial</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>otgala6,otchat6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>otgala6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>01:34:27</td>\n",
       "      <td>saramatarile</td>\n",
       "      <td>parece frivolo dejeis darle abrazo intimidad p...</td>\n",
       "      <td>soyanneot2020,ot_oficial</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>otgala6,otchat6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>otchat6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>01:28:51</td>\n",
       "      <td>carmen_abcdd</td>\n",
       "      <td>gerard anne voy olvidar cuanto salga primero v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>otgala6,otchat6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>otgala6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>01:28:51</td>\n",
       "      <td>carmen_abcdd</td>\n",
       "      <td>gerard anne voy olvidar cuanto salga primero v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>otgala6,otchat6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>otchat6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>289 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time        username  \\\n",
       "0    23:50:56    nanoflores75   \n",
       "1    23:50:48       miguelsvl   \n",
       "2    23:50:02       justseero   \n",
       "3    23:48:01    pisanijavier   \n",
       "4    23:46:38      fcolopezm_   \n",
       "..        ...             ...   \n",
       "214  01:39:06  ostwaldguillen   \n",
       "217  01:34:27    saramatarile   \n",
       "217  01:34:27    saramatarile   \n",
       "219  01:28:51    carmen_abcdd   \n",
       "219  01:28:51    carmen_abcdd   \n",
       "\n",
       "                                                 tweet  \\\n",
       "0    yosoydelosmillonesdemisterchip datos puros dur...   \n",
       "1    seguir viendo datos reirme contestaciones das ...   \n",
       "2                 datos yosoydelosmillonesdemisterchip   \n",
       "3    yosoydelosmillonesdemisterchip informacion pod...   \n",
       "4    coronavirus españa positivo canarias cuatro ca...   \n",
       "..                                                 ...   \n",
       "214  maialen favorita maialen favorita maialen favo...   \n",
       "217  parece frivolo dejeis darle abrazo intimidad p...   \n",
       "217  parece frivolo dejeis darle abrazo intimidad p...   \n",
       "219  gerard anne voy olvidar cuanto salga primero v...   \n",
       "219  gerard anne voy olvidar cuanto salga primero v...   \n",
       "\n",
       "                     mentions urls photos replies_count retweets_count  \\\n",
       "0                         NaN    0      0             0              0   \n",
       "1                         NaN    0      0             0              0   \n",
       "2                         NaN    0      0             0              0   \n",
       "3                         NaN    0      0             0              0   \n",
       "4                         NaN    1      0             0              0   \n",
       "..                        ...  ...    ...           ...            ...   \n",
       "214                       NaN    0      1             2             33   \n",
       "217  soyanneot2020,ot_oficial    0      0             0              1   \n",
       "217  soyanneot2020,ot_oficial    0      0             0              1   \n",
       "219                       NaN    0      0             1              6   \n",
       "219                       NaN    0      0             1              6   \n",
       "\n",
       "    likes_count                         hashtags retweet video  \\\n",
       "0             0  yosoydelos3millonesdemisterchip       0     0   \n",
       "1             0  yosoydelos3millonesdemisterchip       0     0   \n",
       "2             0  yosoydelos3millonesdemisterchip       0     0   \n",
       "3             0  yosoydelos3millonesdemisterchip       0     0   \n",
       "4             0                      coronavirus       0     0   \n",
       "..          ...                              ...     ...   ...   \n",
       "214         139                  otgala6,otchat6       0     1   \n",
       "217           2                  otgala6,otchat6       0     0   \n",
       "217           2                  otgala6,otchat6       0     0   \n",
       "219          20                  otgala6,otchat6       0     0   \n",
       "219          20                  otgala6,otchat6       0     0   \n",
       "\n",
       "           reply_to hours day month                           trends  \n",
       "0               NaN    23  24     2  yosoydelos3millonesdemisterchip  \n",
       "1               NaN    23  24     2  yosoydelos3millonesdemisterchip  \n",
       "2    2010MisterChip    23  24     2  yosoydelos3millonesdemisterchip  \n",
       "3               NaN    23  24     2  yosoydelos3millonesdemisterchip  \n",
       "4               NaN    23  24     2                      coronavirus  \n",
       "..              ...   ...  ..   ...                              ...  \n",
       "214             NaN     1  24     2                          otchat6  \n",
       "217             NaN     1  24     2                          otgala6  \n",
       "217             NaN     1  24     2                          otchat6  \n",
       "219             NaN     1  24     2                          otgala6  \n",
       "219             NaN     1  24     2                          otchat6  \n",
       "\n",
       "[289 rows x 17 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_h = pd.concat([df_h, df_v])\n",
    "df_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF_P (df de las palabras claves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df_p = df[df.hashtags == \"nan\"]\n",
    "df_p.drop(df_p.columns[0], axis = 1, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df_p.reset_index()\n",
    "df_p.drop(df_p.columns[0], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p[\"trends\"] = [[p for p in palabra.split(\" \") if p in palabras_no_tendencias] for palabra in df_p.tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_drop = list()\n",
    "for num, trend in enumerate(df_p.trends):\n",
    "    if trend == []:\n",
    "        indices_drop.append(num)\n",
    "        \n",
    "df_p.drop(indices_drop, inplace = True)\n",
    "\n",
    "df_p = df_p.reset_index()\n",
    "df_p.drop(df_p.columns[0], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>mentions</th>\n",
       "      <th>urls</th>\n",
       "      <th>photos</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet</th>\n",
       "      <th>video</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>hours</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>trends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23:59:56</td>\n",
       "      <td>nuriafr16</td>\n",
       "      <td>lightmyfire ver</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>__Lightmyfire</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[ver]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23:59:52</td>\n",
       "      <td>inigojouron</td>\n",
       "      <td>dios infectemos madrid mueran volvemos llevarn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MMissingx</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[madrid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23:59:31</td>\n",
       "      <td>diecinuevelu</td>\n",
       "      <td>quiero mas ser exploto jodernos vida nuevo mal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[mas, ser]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23:59:28</td>\n",
       "      <td>old_atarian</td>\n",
       "      <td>viendo serie “sabrina” gratamente sorprendido ...</td>\n",
       "      <td>netflixes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[tambien]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23:59:27</td>\n",
       "      <td>_airwin_</td>\n",
       "      <td>siempre placer volver auditorio navional mucha...</td>\n",
       "      <td>fundscherzo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[siempre, gracias]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4781</th>\n",
       "      <td>01:00:45</td>\n",
       "      <td>mrhearoh</td>\n",
       "      <td>noemi terrorista coño haces salva alguien dios...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[hace]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4782</th>\n",
       "      <td>01:00:44</td>\n",
       "      <td>yatzirylunaa</td>\n",
       "      <td>doing homework while studying abroad such slap...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[the]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4783</th>\n",
       "      <td>01:00:26</td>\n",
       "      <td>martianthen</td>\n",
       "      <td>creo estrategia gente vote mas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[gente, mas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4784</th>\n",
       "      <td>01:00:13</td>\n",
       "      <td>alejandroxx__</td>\n",
       "      <td>ver entiende alguien vezzzz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[ver]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>01:00:10</td>\n",
       "      <td>pantxisko</td>\n",
       "      <td>chaval victima bullying reacciona asi harto si...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mnicolas83</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[asi]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4786 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          time       username  \\\n",
       "0     23:59:56      nuriafr16   \n",
       "1     23:59:52    inigojouron   \n",
       "2     23:59:31   diecinuevelu   \n",
       "3     23:59:28    old_atarian   \n",
       "4     23:59:27       _airwin_   \n",
       "...        ...            ...   \n",
       "4781  01:00:45       mrhearoh   \n",
       "4782  01:00:44   yatzirylunaa   \n",
       "4783  01:00:26    martianthen   \n",
       "4784  01:00:13  alejandroxx__   \n",
       "4785  01:00:10      pantxisko   \n",
       "\n",
       "                                                  tweet     mentions  urls  \\\n",
       "0                                       lightmyfire ver          NaN   0.0   \n",
       "1     dios infectemos madrid mueran volvemos llevarn...          NaN   0.0   \n",
       "2     quiero mas ser exploto jodernos vida nuevo mal...          NaN   0.0   \n",
       "3     viendo serie “sabrina” gratamente sorprendido ...    netflixes   0.0   \n",
       "4     siempre placer volver auditorio navional mucha...  fundscherzo   0.0   \n",
       "...                                                 ...          ...   ...   \n",
       "4781  noemi terrorista coño haces salva alguien dios...          NaN   0.0   \n",
       "4782  doing homework while studying abroad such slap...          NaN   0.0   \n",
       "4783                     creo estrategia gente vote mas          NaN   0.0   \n",
       "4784                        ver entiende alguien vezzzz          NaN   0.0   \n",
       "4785  chaval victima bullying reacciona asi harto si...          NaN   0.0   \n",
       "\n",
       "     photos  replies_count  retweets_count  likes_count hashtags  retweet  \\\n",
       "0         0            0.0             0.0          1.0      nan      0.0   \n",
       "1         0            1.0             0.0          1.0      nan      0.0   \n",
       "2         0            1.0             1.0          1.0      nan      0.0   \n",
       "3         0            0.0             0.0          1.0      nan      0.0   \n",
       "4         1            0.0             1.0          4.0      nan      0.0   \n",
       "...     ...            ...             ...          ...      ...      ...   \n",
       "4781      0            1.0             0.0          1.0      nan      0.0   \n",
       "4782      0            1.0             1.0         13.0      nan      0.0   \n",
       "4783      0            0.0             0.0          0.0      nan      0.0   \n",
       "4784      0            0.0             0.0          0.0      nan      0.0   \n",
       "4785      0            0.0             0.0          0.0      nan      0.0   \n",
       "\n",
       "      video       reply_to  hours   day  month              trends  \n",
       "0       0.0  __Lightmyfire   23.0  24.0    2.0               [ver]  \n",
       "1       0.0      MMissingx   23.0  24.0    2.0            [madrid]  \n",
       "2       0.0            NaN   23.0  24.0    2.0          [mas, ser]  \n",
       "3       0.0            NaN   23.0  24.0    2.0           [tambien]  \n",
       "4       1.0            NaN   23.0  24.0    2.0  [siempre, gracias]  \n",
       "...     ...            ...    ...   ...    ...                 ...  \n",
       "4781    0.0            NaN    1.0  24.0    2.0              [hace]  \n",
       "4782    0.0            NaN    1.0  24.0    2.0               [the]  \n",
       "4783    0.0            NaN    1.0  24.0    2.0        [gente, mas]  \n",
       "4784    0.0            NaN    1.0  24.0    2.0               [ver]  \n",
       "4785    0.0     mnicolas83    1.0  24.0    2.0               [asi]  \n",
       "\n",
       "[4786 rows x 17 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_para_clonar = list()\n",
    "for num, t in enumerate(df_p.trends):\n",
    "    if len(t) > 1:\n",
    "        indices_para_clonar.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_indices = {indice : [len(trends), trends] for indice, trends in zip(indices_para_clonar, df_p.loc[indices_para_clonar].trends)}\n",
    "\n",
    "df_v = pd.DataFrame(columns = df_p.columns)\n",
    "\n",
    "for key in dic_indices.keys():\n",
    "    for time in range(dic_indices[key][0]):\n",
    "        df_d = pd.DataFrame(df_p.loc[key]).T\n",
    "        df_d.drop(df_d.columns[-1], axis = 1, inplace = True)\n",
    "        df_d[\"trends\"] = dic_indices[key][1][time]\n",
    "        df_v = pd.concat([df_v, df_d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.drop(indices_para_clonar, inplace = True)\n",
    "\n",
    "df_p = df_p.reset_index()\n",
    "df_p.drop(df_p.columns[0], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.trends = df_p.trends.apply(lambda x : x[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.concat([df_p, df_v])\n",
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_procesado = pd.concat([df_h, df_p])\n",
    "df_procesado.to_csv(\"tweet_{}_notendencia_preprocesado.csv\".format(FECHA), sep = \";\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
